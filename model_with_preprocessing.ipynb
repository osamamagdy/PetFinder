{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting the libraries\n",
    "import pyspark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 3 features:  'Type', 'Age', 'Breed1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read the data and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data:  16870\n",
      "Size of the test data:  2999\n"
     ]
    }
   ],
   "source": [
    "# To work with spark we need to create a spark session\n",
    "# Need to instal java\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local=[*]\").appName('petfinder').getOrCreate()\n",
    "\n",
    "########### For the train dataset\n",
    "# Read a dataset with spark\n",
    "df_spark = spark.read.csv('./train_balanced_corr.csv', header=True, inferSchema=True)\n",
    "# Header = True, inferSchema = True, means that the first row is the header and the schema is inferred (if schema is not inferred, all columns will be read as string)\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark = df_spark.withColumn(\"AdoptionSpeed\", df_spark[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "\n",
    "\n",
    "############ For the test dataset\n",
    "# Read a dataset with spark\n",
    "df_spark_test = spark.read.csv('./test_split_corr.csv', header=True, inferSchema=True)\n",
    "# Header = True, inferSchema = True, means that the first row is the header and the schema is inferred (if schema is not inferred, all columns will be read as string)\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark_test = df_spark_test.withColumn(\"AdoptionSpeed\", df_spark_test[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "\n",
    "\n",
    "## Print size of the data\n",
    "print(\"Size of the training data: \", df_spark.count())\n",
    "print(\"Size of the test data: \", df_spark_test.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+-------------+\n",
      "|Breed1|Age|Type|AdoptionSpeed|\n",
      "+------+---+----+-------------+\n",
      "|   307|  2|   1|            1|\n",
      "|   307| 36|   1|            4|\n",
      "|   179|  2|   1|            1|\n",
      "|   265| 27|   2|            4|\n",
      "|   307|  2|   1|            1|\n",
      "|   266| 29|   2|            4|\n",
      "|    83| 36|   1|            1|\n",
      "|   307| 24|   1|            3|\n",
      "|   307| 21|   1|            4|\n",
      "|   307| 29|   1|            3|\n",
      "|   307|  3|   1|            2|\n",
      "|    60|120|   1|            2|\n",
      "|   145| 18|   1|            2|\n",
      "|   254| 24|   2|            3|\n",
      "|   205| 36|   1|            2|\n",
      "|   307|  2|   1|            3|\n",
      "|   266| 12|   2|            3|\n",
      "|   307|  3|   1|            2|\n",
      "|   283| 48|   2|            4|\n",
      "|   265|  3|   2|            2|\n",
      "+------+---+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---+----+-------------+\n",
      "|Breed1|Age|Type|AdoptionSpeed|\n",
      "+------+---+----+-------------+\n",
      "|   265|  7|   2|            4|\n",
      "|   266| 24|   2|            4|\n",
      "|   266| 12|   2|            2|\n",
      "|   195| 60|   1|            1|\n",
      "|   266|  3|   2|            4|\n",
      "|   307| 12|   1|            2|\n",
      "|   218| 16|   1|            4|\n",
      "|   285| 24|   2|            3|\n",
      "|   266| 36|   2|            4|\n",
      "|   108|  8|   1|            2|\n",
      "|   307| 36|   1|            3|\n",
      "|   307| 15|   1|            4|\n",
      "|    20| 60|   1|            2|\n",
      "|   307| 18|   1|            3|\n",
      "|   307|  1|   1|            1|\n",
      "|   252| 60|   2|            3|\n",
      "|   285|  6|   2|            2|\n",
      "|    88|  2|   1|            3|\n",
      "|   307|  3|   1|            1|\n",
      "|   307|  1|   1|            1|\n",
      "+------+---+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### For the train dataset\n",
    "\n",
    "## Drop rows with missing values\n",
    "# df_spark.na.drop(how='all', thresh=10).show() \n",
    "    ### how='any' means drop rows with any missing value, how='all' means drop rows whose all values are missing\n",
    "    ### thresh=10 means drop rows whose number of missing values is greater than 10\n",
    "    ### subset=['Age'] means drop rows whose 'Age' value is missing\n",
    "df_spark = df_spark.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "## Fill missing values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean_val = df_spark.select(mean(df_spark['Age'])).collect()\n",
    "mean_age = mean_val[0][0]\n",
    "df_spark.na.fill(mean_age, subset=['Age']).show()\n",
    "\n",
    "########### For the test dataset\n",
    "\n",
    "df_spark_test = df_spark_test.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "## Fill missing values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean_val = df_spark_test.select(mean(df_spark_test['Age'])).collect()\n",
    "mean_age = mean_val[0][0]\n",
    "df_spark_test.na.fill(mean_age, subset=['Age']).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark MLlib to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+-------------+----------------+\n",
      "|Breed1|Age|Type|AdoptionSpeed|        features|\n",
      "+------+---+----+-------------+----------------+\n",
      "|   307|  2|   1|            1| [307.0,2.0,1.0]|\n",
      "|   307| 36|   1|            4|[307.0,36.0,1.0]|\n",
      "|   179|  2|   1|            1| [179.0,2.0,1.0]|\n",
      "|   265| 27|   2|            4|[265.0,27.0,2.0]|\n",
      "|   307|  2|   1|            1| [307.0,2.0,1.0]|\n",
      "|   266| 29|   2|            4|[266.0,29.0,2.0]|\n",
      "|    83| 36|   1|            1| [83.0,36.0,1.0]|\n",
      "|   307| 24|   1|            3|[307.0,24.0,1.0]|\n",
      "|   307| 21|   1|            4|[307.0,21.0,1.0]|\n",
      "|   307| 29|   1|            3|[307.0,29.0,1.0]|\n",
      "|   307|  3|   1|            2| [307.0,3.0,1.0]|\n",
      "|    60|120|   1|            2|[60.0,120.0,1.0]|\n",
      "|   145| 18|   1|            2|[145.0,18.0,1.0]|\n",
      "|   254| 24|   2|            3|[254.0,24.0,2.0]|\n",
      "|   205| 36|   1|            2|[205.0,36.0,1.0]|\n",
      "|   307|  2|   1|            3| [307.0,2.0,1.0]|\n",
      "|   266| 12|   2|            3|[266.0,12.0,2.0]|\n",
      "|   307|  3|   1|            2| [307.0,3.0,1.0]|\n",
      "|   283| 48|   2|            4|[283.0,48.0,2.0]|\n",
      "|   265|  3|   2|            2| [265.0,3.0,2.0]|\n",
      "+------+---+----+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---+----+-------------+----------------+\n",
      "|Breed1|Age|Type|AdoptionSpeed|        features|\n",
      "+------+---+----+-------------+----------------+\n",
      "|   265|  7|   2|            4| [265.0,7.0,2.0]|\n",
      "|   266| 24|   2|            4|[266.0,24.0,2.0]|\n",
      "|   266| 12|   2|            2|[266.0,12.0,2.0]|\n",
      "|   195| 60|   1|            1|[195.0,60.0,1.0]|\n",
      "|   266|  3|   2|            4| [266.0,3.0,2.0]|\n",
      "|   307| 12|   1|            2|[307.0,12.0,1.0]|\n",
      "|   218| 16|   1|            4|[218.0,16.0,1.0]|\n",
      "|   285| 24|   2|            3|[285.0,24.0,2.0]|\n",
      "|   266| 36|   2|            4|[266.0,36.0,2.0]|\n",
      "|   108|  8|   1|            2| [108.0,8.0,1.0]|\n",
      "|   307| 36|   1|            3|[307.0,36.0,1.0]|\n",
      "|   307| 15|   1|            4|[307.0,15.0,1.0]|\n",
      "|    20| 60|   1|            2| [20.0,60.0,1.0]|\n",
      "|   307| 18|   1|            3|[307.0,18.0,1.0]|\n",
      "|   307|  1|   1|            1| [307.0,1.0,1.0]|\n",
      "|   252| 60|   2|            3|[252.0,60.0,2.0]|\n",
      "|   285|  6|   2|            2| [285.0,6.0,2.0]|\n",
      "|    88|  2|   1|            3|  [88.0,2.0,1.0]|\n",
      "|   307|  3|   1|            1| [307.0,3.0,1.0]|\n",
      "|   307|  1|   1|            1| [307.0,1.0,1.0]|\n",
      "+------+---+----+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, collect the features in a single column\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#### For the train dataset\n",
    "featureassemble = VectorAssembler(inputCols=['Breed1','Age','Type'], outputCol='features')\n",
    "output = featureassemble.transform(df_spark) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "output.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfeatureassemble = VectorAssembler(inputCols=['Breed1','Age','Type'], outputCol='features')\n",
    "testoutput = testfeatureassemble.transform(df_spark_test) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "testoutput.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|        features|AdoptionSpeed|\n",
      "+----------------+-------------+\n",
      "| [307.0,2.0,1.0]|            1|\n",
      "|[307.0,36.0,1.0]|            4|\n",
      "| [179.0,2.0,1.0]|            1|\n",
      "|[265.0,27.0,2.0]|            4|\n",
      "| [307.0,2.0,1.0]|            1|\n",
      "|[266.0,29.0,2.0]|            4|\n",
      "| [83.0,36.0,1.0]|            1|\n",
      "|[307.0,24.0,1.0]|            3|\n",
      "|[307.0,21.0,1.0]|            4|\n",
      "|[307.0,29.0,1.0]|            3|\n",
      "| [307.0,3.0,1.0]|            2|\n",
      "|[60.0,120.0,1.0]|            2|\n",
      "|[145.0,18.0,1.0]|            2|\n",
      "|[254.0,24.0,2.0]|            3|\n",
      "|[205.0,36.0,1.0]|            2|\n",
      "| [307.0,2.0,1.0]|            3|\n",
      "|[266.0,12.0,2.0]|            3|\n",
      "| [307.0,3.0,1.0]|            2|\n",
      "|[283.0,48.0,2.0]|            4|\n",
      "| [265.0,3.0,2.0]|            2|\n",
      "+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-------------+\n",
      "|        features|AdoptionSpeed|\n",
      "+----------------+-------------+\n",
      "| [265.0,7.0,2.0]|            4|\n",
      "|[266.0,24.0,2.0]|            4|\n",
      "|[266.0,12.0,2.0]|            2|\n",
      "|[195.0,60.0,1.0]|            1|\n",
      "| [266.0,3.0,2.0]|            4|\n",
      "|[307.0,12.0,1.0]|            2|\n",
      "|[218.0,16.0,1.0]|            4|\n",
      "|[285.0,24.0,2.0]|            3|\n",
      "|[266.0,36.0,2.0]|            4|\n",
      "| [108.0,8.0,1.0]|            2|\n",
      "|[307.0,36.0,1.0]|            3|\n",
      "|[307.0,15.0,1.0]|            4|\n",
      "| [20.0,60.0,1.0]|            2|\n",
      "|[307.0,18.0,1.0]|            3|\n",
      "| [307.0,1.0,1.0]|            1|\n",
      "|[252.0,60.0,2.0]|            3|\n",
      "| [285.0,6.0,2.0]|            2|\n",
      "|  [88.0,2.0,1.0]|            3|\n",
      "| [307.0,3.0,1.0]|            1|\n",
      "| [307.0,1.0,1.0]|            1|\n",
      "+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the features and the target column\n",
    "\n",
    "#### For the train dataset\n",
    "finalized_data = output.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "finalized_data.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfinalized_data = testoutput.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "testfinalized_data.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "|        features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "| [265.0,7.0,2.0]|            4|[0.21852222769436...|[0.24594501729175...|       0.0|\n",
      "|[266.0,24.0,2.0]|            4|[0.21091546173330...|[0.24452052550620...|       0.0|\n",
      "|[266.0,12.0,2.0]|            2|[0.21328101316232...|[0.24531022067589...|       0.0|\n",
      "|[195.0,60.0,1.0]|            1|[0.02040640887454...|[0.19880089651182...|       4.0|\n",
      "| [266.0,3.0,2.0]|            4|[0.21505517673408...|[0.24453461523033...|       0.0|\n",
      "|[307.0,12.0,1.0]|            2|[-0.4467550129775...|[0.12267623478828...|       4.0|\n",
      "|[218.0,16.0,1.0]|            4|[-0.0687979689279...|[0.18565850870324...|       3.0|\n",
      "|[285.0,24.0,2.0]|            3|[0.13005966777085...|[0.22530280619895...|       4.0|\n",
      "|[266.0,36.0,2.0]|            4|[0.20854991030428...|[0.24150748150536...|       4.0|\n",
      "| [108.0,8.0,1.0]|            2|[0.40089155671957...|[0.27045693183075...|       0.0|\n",
      "|[307.0,36.0,1.0]|            3|[-0.4514861158355...|[0.11561227728769...|       4.0|\n",
      "|[307.0,15.0,1.0]|            4|[-0.4473464008347...|[0.12205561826722...|       4.0|\n",
      "| [20.0,60.0,1.0]|            2|[0.76513082694975...|[0.37992074510452...|       0.0|\n",
      "|[307.0,18.0,1.0]|            3|[-0.4479377886920...|[0.12135902382152...|       4.0|\n",
      "| [307.0,1.0,1.0]|            1|[-0.4445865908342...|[0.12430448146949...|       3.0|\n",
      "|[252.0,60.0,2.0]|            3|[0.26339676089227...|[0.24536736172959...|       4.0|\n",
      "| [285.0,6.0,2.0]|            2|[0.13360799491437...|[0.22748000118850...|       0.0|\n",
      "|  [88.0,2.0,1.0]|            3|[0.48718569449982...|[0.28215701802101...|       1.0|\n",
      "| [307.0,3.0,1.0]|            1|[-0.4449808494057...|[0.12408352119394...|       4.0|\n",
      "| [307.0,1.0,1.0]|            1|[-0.4445865908342...|[0.12430448146949...|       3.0|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       0.0|\n",
      "|            4|       0.0|\n",
      "|            2|       0.0|\n",
      "|            1|       4.0|\n",
      "|            4|       0.0|\n",
      "|            2|       4.0|\n",
      "|            4|       3.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            1|       3.0|\n",
      "|            3|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       1.0|\n",
      "|            1|       4.0|\n",
      "|            1|       3.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.193\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       3|281| 30|166|165|\n",
      "|                       0| 47|  8| 16| 10|\n",
      "|                       1|364| 40|163| 52|\n",
      "|                       4|323| 16|154|330|\n",
      "|                       2|408| 36|236|154|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = LogisticRegression\n",
    "classifier = LogisticRegression(labelCol='AdoptionSpeed').fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.evaluate(test_data) # Evaluate the model on the validation data\n",
    "results.predictions.show() # Show the predictions\n",
    "results.predictions.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results.predictions)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.predictions.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "|        features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "| [265.0,7.0,2.0]|            4|[388.0,258.0,317....|[0.19715447154471...|       4.0|\n",
      "|[266.0,24.0,2.0]|            4|[388.0,258.0,317....|[0.19715447154471...|       4.0|\n",
      "|[266.0,12.0,2.0]|            2|[388.0,258.0,317....|[0.19715447154471...|       4.0|\n",
      "|[195.0,60.0,1.0]|            1|[380.0,195.0,147....|[0.39915966386554...|       0.0|\n",
      "| [266.0,3.0,2.0]|            4|[475.0,657.0,581....|[0.19467213114754...|       1.0|\n",
      "|[307.0,12.0,1.0]|            2|[163.0,156.0,291....|[0.08101391650099...|       4.0|\n",
      "|[218.0,16.0,1.0]|            4|[388.0,258.0,317....|[0.19715447154471...|       4.0|\n",
      "|[285.0,24.0,2.0]|            3|[214.0,128.0,98.0...|[0.31845238095238...|       0.0|\n",
      "|[266.0,36.0,2.0]|            4|[388.0,258.0,317....|[0.19715447154471...|       4.0|\n",
      "| [108.0,8.0,1.0]|            2|[25.0,56.0,52.0,5...|[0.11061946902654...|       3.0|\n",
      "|[307.0,36.0,1.0]|            3|[163.0,156.0,291....|[0.08101391650099...|       4.0|\n",
      "|[307.0,15.0,1.0]|            4|[163.0,156.0,291....|[0.08101391650099...|       4.0|\n",
      "| [20.0,60.0,1.0]|            2|[236.0,206.0,154....|[0.26077348066298...|       0.0|\n",
      "|[307.0,18.0,1.0]|            3|[163.0,156.0,291....|[0.08101391650099...|       4.0|\n",
      "| [307.0,1.0,1.0]|            1|[167.0,247.0,357....|[0.13180741910023...|       2.0|\n",
      "|[252.0,60.0,2.0]|            3|[388.0,258.0,317....|[0.19715447154471...|       4.0|\n",
      "| [285.0,6.0,2.0]|            2|[214.0,128.0,98.0...|[0.31845238095238...|       0.0|\n",
      "|  [88.0,2.0,1.0]|            3|[122.0,176.0,133....|[0.20469798657718...|       1.0|\n",
      "| [307.0,3.0,1.0]|            1|[45.0,145.0,200.0...|[0.05696202531645...|       3.0|\n",
      "| [307.0,1.0,1.0]|            1|[167.0,247.0,357....|[0.13180741910023...|       2.0|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            1|       0.0|\n",
      "|            4|       1.0|\n",
      "|            2|       4.0|\n",
      "|            4|       4.0|\n",
      "|            3|       0.0|\n",
      "|            4|       4.0|\n",
      "|            2|       3.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            1|       2.0|\n",
      "|            3|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       1.0|\n",
      "|            1|       3.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.298\n",
      "+------------------------+---+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+---+\n",
      "|                       3|143|142|127| 70|160|\n",
      "|                       0| 22| 27| 15|  3| 14|\n",
      "|                       1|208|160|133| 41| 77|\n",
      "|                       4|168| 67|132| 78|378|\n",
      "|                       2|218|177|206| 68|165|\n",
      "+------------------------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = DecisionTreeClassifier( labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "|        features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "| [265.0,7.0,2.0]|            4|[4.66462762344093...|[0.23323138117204...|       4.0|\n",
      "|[266.0,24.0,2.0]|            4|[3.90832354413122...|[0.19541617720656...|       4.0|\n",
      "|[266.0,12.0,2.0]|            2|[3.80982603011887...|[0.19049130150594...|       4.0|\n",
      "|[195.0,60.0,1.0]|            1|[6.90513437153496...|[0.34525671857674...|       0.0|\n",
      "| [266.0,3.0,2.0]|            4|[4.14015478728268...|[0.20700773936413...|       1.0|\n",
      "|[307.0,12.0,1.0]|            2|[1.87071354752533...|[0.09353567737626...|       4.0|\n",
      "|[218.0,16.0,1.0]|            4|[4.40701208522221...|[0.22035060426111...|       4.0|\n",
      "|[285.0,24.0,2.0]|            3|[4.99401987424813...|[0.24970099371240...|       0.0|\n",
      "|[266.0,36.0,2.0]|            4|[3.63240680226321...|[0.18162034011316...|       4.0|\n",
      "| [108.0,8.0,1.0]|            2|[4.96694017547938...|[0.24834700877396...|       0.0|\n",
      "|[307.0,36.0,1.0]|            3|[1.71854088262213...|[0.08592704413110...|       4.0|\n",
      "|[307.0,15.0,1.0]|            4|[1.74586772623251...|[0.08729338631162...|       4.0|\n",
      "| [20.0,60.0,1.0]|            2|[4.75876274592556...|[0.23793813729627...|       1.0|\n",
      "|[307.0,18.0,1.0]|            3|[1.74586772623251...|[0.08729338631162...|       4.0|\n",
      "| [307.0,1.0,1.0]|            1|[2.61943161639927...|[0.13097158081996...|       2.0|\n",
      "|[252.0,60.0,2.0]|            3|[4.24630540486596...|[0.21231527024329...|       4.0|\n",
      "| [285.0,6.0,2.0]|            2|[7.13779436500756...|[0.35688971825037...|       0.0|\n",
      "|  [88.0,2.0,1.0]|            3|[4.95837851266197...|[0.24791892563309...|       1.0|\n",
      "| [307.0,3.0,1.0]|            1|[1.55890077361058...|[0.07794503868052...|       3.0|\n",
      "| [307.0,1.0,1.0]|            1|[2.61943161639927...|[0.13097158081996...|       2.0|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            1|       0.0|\n",
      "|            4|       1.0|\n",
      "|            2|       4.0|\n",
      "|            4|       4.0|\n",
      "|            3|       0.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       1.0|\n",
      "|            3|       4.0|\n",
      "|            1|       2.0|\n",
      "|            3|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       1.0|\n",
      "|            1|       3.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.304\n",
      "+------------------------+---+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+---+\n",
      "|                       3|132|145|127| 49|189|\n",
      "|                       0| 25| 23| 15|  1| 17|\n",
      "|                       1|168|197|135| 17|102|\n",
      "|                       4|117| 88|133| 53|432|\n",
      "|                       2|156|219|206| 43|210|\n",
      "+------------------------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = RandomForestClassifier(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "|        features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "| [265.0,7.0,2.0]|            4|[-46.974540850384...|[0.14402667045828...|       2.0|\n",
      "|[266.0,24.0,2.0]|            4|[-101.24812790298...|[0.17873348172516...|       4.0|\n",
      "|[266.0,12.0,2.0]|            2|[-62.971521419971...|[0.25457037136800...|       0.0|\n",
      "|[195.0,60.0,1.0]|            1|[-207.53567000969...|[0.00110149909586...|       4.0|\n",
      "| [266.0,3.0,2.0]|            4|[-34.264066557712...|[0.06978615073014...|       2.0|\n",
      "|[307.0,12.0,1.0]|            2|[-59.849431997550...|[0.19599104897496...|       3.0|\n",
      "|[218.0,16.0,1.0]|            4|[-68.301187210301...|[0.24240190122342...|       4.0|\n",
      "|[285.0,24.0,2.0]|            3|[-102.16762406796...|[0.19066671534541...|       4.0|\n",
      "|[266.0,36.0,2.0]|            4|[-139.52473438599...|[0.04484194761579...|       4.0|\n",
      "| [108.0,8.0,1.0]|            2|[-37.460050705060...|[0.26457353006534...|       4.0|\n",
      "|[307.0,36.0,1.0]|            3|[-136.40264496357...|[0.04475177372986...|       4.0|\n",
      "|[307.0,15.0,1.0]|            4|[-69.418583618303...|[0.24059649742956...|       4.0|\n",
      "| [20.0,60.0,1.0]|            2|[-199.06662638485...|[4.60390919166669...|       4.0|\n",
      "|[307.0,18.0,1.0]|            3|[-78.987735239056...|[0.24699024290453...|       4.0|\n",
      "| [307.0,1.0,1.0]|            1|[-24.762542721456...|[0.02910676910243...|       2.0|\n",
      "|[252.0,60.0,2.0]|            3|[-215.40042386203...|[0.00179573503378...|       4.0|\n",
      "| [285.0,6.0,2.0]|            2|[-44.752714343448...|[0.10597395072662...|       2.0|\n",
      "|  [88.0,2.0,1.0]|            3|[-17.353856763573...|[0.18378945931450...|       1.0|\n",
      "| [307.0,3.0,1.0]|            1|[-31.141977135291...|[0.04467685982069...|       2.0|\n",
      "| [307.0,1.0,1.0]|            1|[-24.762542721456...|[0.02910676910243...|       2.0|\n",
      "+----------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       2.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            1|       4.0|\n",
      "|            4|       2.0|\n",
      "|            2|       3.0|\n",
      "|            4|       4.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            3|       4.0|\n",
      "|            1|       2.0|\n",
      "|            3|       4.0|\n",
      "|            2|       2.0|\n",
      "|            3|       1.0|\n",
      "|            1|       2.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.235\n",
      "+------------------------+---+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+---+\n",
      "|                       3| 30| 24|432| 25|131|\n",
      "|                       0|  7|  1| 54|  4| 15|\n",
      "|                       1| 23| 18|466|  5|107|\n",
      "|                       4| 47| 25|479| 44|228|\n",
      "|                       2| 29| 22|632| 19|132|\n",
      "+------------------------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = NaiveBayes(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Map Reduce to build Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with spark we need to create a spark session\n",
    "# Need to instal java\n",
    "from pyspark.sql import SparkSession\n",
    "rdd_spark_session = SparkSession.builder.master(\"local=[*]\").config(\"spark.driver.memory\", \"15g\").appName('petfinderMapReduce').getOrCreate()\n",
    "\n",
    "# Read the data\n",
    "df_spark = rdd_spark_session.read.csv('./train_balanced_corr.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Cast the column \"AdoptionSpeed\" to integer\n",
    "df_spark = df_spark.withColumn(\"AdoptionSpeed\", df_spark[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "\n",
    "df_spark = df_spark.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "\n",
    "# Convert the data to RDD\n",
    "rdd_spark = df_spark.rdd\n",
    "\n",
    "# Map the data to count the number of each class\n",
    "rdd_spark_map = rdd_spark.map(lambda x: (x[3], 1))\n",
    "# To work with spark we need to create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 219.0 failed 1 times, most recent failure: Lost task 0.0 in stage 219.0 (TID 171) (LAPTOP-0IJ6E4KR executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21572\\1096153451.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd_spark_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\osama\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3436\u001b[0m         \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3437\u001b[0m         \"\"\"\n\u001b[1;32m-> 3438\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3440\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"RDD[Tuple[K, V]]\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"RDD[K]\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\osama\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\osama\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\osama\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\osama\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 219.0 failed 1 times, most recent failure: Lost task 0.0 in stage 219.0 (TID 171) (LAPTOP-0IJ6E4KR executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the result\n",
    "rdd_spark_map.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
