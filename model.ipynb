{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting the libraries\n",
    "import pyspark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 3 features:  'Type', 'Age', 'Breed1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read the data and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/06 11:17:25 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.17.130 instead (on interface ens33)\n",
      "23/05/06 11:17:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/06 11:17:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/06 11:17:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/06 11:17:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data:  10468\n",
      "Size of the test data:  4525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To work with spark we need to create a spark session\n",
    "# Need to instal java\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('petfinder').getOrCreate()\n",
    "\n",
    "# Importing the dataset and split it into training and test data\n",
    "df_spark, df_spark_test = spark.read.csv('./train.csv', header=True, inferSchema=True).randomSplit([0.7, 0.3])\n",
    "\n",
    "######### For the training data #########\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark = df_spark.withColumn(\"AdoptionSpeed\", df_spark[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "# Select the columns that we need ['Type', 'Age', 'Breed1']\n",
    "df_spark = df_spark.select(['Type', 'Age', 'Breed1', 'AdoptionSpeed'])\n",
    "\n",
    "######### For the test data #########\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark_test = df_spark_test.withColumn(\"AdoptionSpeed\", df_spark_test[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "# Select the columns that we need ['Type', 'Age', 'Breed1']\n",
    "df_spark_test = df_spark_test.select(['Type', 'Age', 'Breed1', 'AdoptionSpeed'])\n",
    "\n",
    "\n",
    "\n",
    "## Print size of the data\n",
    "print(\"Size of the training data: \", df_spark.count())\n",
    "print(\"Size of the test data: \", df_spark_test.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+-------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|\n",
      "+----+---+------+-------------+\n",
      "|   1|  0|   307|            4|\n",
      "|   1|  0|   307|            2|\n",
      "|   1|  0|   307|            4|\n",
      "|   1|  0|   307|            1|\n",
      "|   1|  0|   307|            0|\n",
      "|   1|  0|   307|            4|\n",
      "|   1|  1|   128|            3|\n",
      "|   1|  1|   141|            4|\n",
      "|   1|  1|   173|            0|\n",
      "|   1|  1|   189|            0|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            1|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            4|\n",
      "+----+---+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---+------+-------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|\n",
      "+----+---+------+-------------+\n",
      "|   1|  0|   307|            1|\n",
      "|   1|  0|   307|            2|\n",
      "|   1|  0|   307|            3|\n",
      "|   1|  1|   189|            1|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            1|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            0|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            1|\n",
      "+----+---+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### For the train dataset\n",
    "\n",
    "## Drop rows with missing values\n",
    "# df_spark.na.drop(how='all', thresh=10).show() \n",
    "    ### how='any' means drop rows with any missing value, how='all' means drop rows whose all values are missing\n",
    "    ### thresh=10 means drop rows whose number of missing values is greater than 10\n",
    "    ### subset=['Age'] means drop rows whose 'Age' value is missing\n",
    "df_spark = df_spark.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "## Fill missing values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean_val = df_spark.select(mean(df_spark['Age'])).collect()\n",
    "mean_age = mean_val[0][0]\n",
    "df_spark.na.fill(mean_age, subset=['Age']).show()\n",
    "\n",
    "########### For the test dataset\n",
    "\n",
    "df_spark_test = df_spark_test.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "## Fill missing values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean_val = df_spark_test.select(mean(df_spark_test['Age'])).collect()\n",
    "mean_age = mean_val[0][0]\n",
    "df_spark_test.na.fill(mean_age, subset=['Age']).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark MLlib to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+-------------+---------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|       features|\n",
      "+----+---+------+-------------+---------------+\n",
      "|   1|  0|   307|            4|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            2|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            4|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            1|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            0|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            4|[307.0,0.0,1.0]|\n",
      "|   1|  1|   128|            3|[128.0,1.0,1.0]|\n",
      "|   1|  1|   141|            4|[141.0,1.0,1.0]|\n",
      "|   1|  1|   173|            0|[173.0,1.0,1.0]|\n",
      "|   1|  1|   189|            0|[189.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "+----+---+------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---+------+-------------+---------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|       features|\n",
      "+----+---+------+-------------+---------------+\n",
      "|   1|  0|   307|            1|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            2|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            3|[307.0,0.0,1.0]|\n",
      "|   1|  1|   189|            1|[189.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            0|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "+----+---+------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, collect the features in a single column\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#### For the train dataset\n",
    "featureassemble = VectorAssembler(inputCols=['Breed1','Age','Type'], outputCol='features')\n",
    "output = featureassemble.transform(df_spark) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "output.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfeatureassemble = VectorAssembler(inputCols=['Breed1','Age','Type'], outputCol='features')\n",
    "testoutput = testfeatureassemble.transform(df_spark_test) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "testoutput.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|       features|AdoptionSpeed|\n",
      "+---------------+-------------+\n",
      "|[307.0,0.0,1.0]|            4|\n",
      "|[307.0,0.0,1.0]|            2|\n",
      "|[307.0,0.0,1.0]|            4|\n",
      "|[307.0,0.0,1.0]|            1|\n",
      "|[307.0,0.0,1.0]|            0|\n",
      "|[307.0,0.0,1.0]|            4|\n",
      "|[128.0,1.0,1.0]|            3|\n",
      "|[141.0,1.0,1.0]|            4|\n",
      "|[173.0,1.0,1.0]|            0|\n",
      "|[189.0,1.0,1.0]|            0|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-------------+\n",
      "|       features|AdoptionSpeed|\n",
      "+---------------+-------------+\n",
      "|[307.0,0.0,1.0]|            1|\n",
      "|[307.0,0.0,1.0]|            2|\n",
      "|[307.0,0.0,1.0]|            3|\n",
      "|[189.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            0|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the features and the target column\n",
    "\n",
    "#### For the train dataset\n",
    "finalized_data = output.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "finalized_data.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfinalized_data = testoutput.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "testfinalized_data.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/06 11:17:39 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            1|[2.55839898909816...|[0.01613506826763...|       4.0|\n",
      "|[307.0,0.0,1.0]|            2|[2.55839898909816...|[0.01613506826763...|       4.0|\n",
      "|[307.0,0.0,1.0]|            3|[2.55839898909816...|[0.01613506826763...|       4.0|\n",
      "|[189.0,1.0,1.0]|            1|[2.70863653112467...|[0.02592680739935...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            4|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            3|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            3|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            1|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            0|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            4|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            4|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            3|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "|[307.0,1.0,1.0]|            1|[2.56234084180089...|[0.01611716885891...|       4.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            1|       4.0|\n",
      "|            2|       4.0|\n",
      "|            3|       4.0|\n",
      "|            1|       2.0|\n",
      "|            2|       4.0|\n",
      "|            4|       4.0|\n",
      "|            3|       4.0|\n",
      "|            3|       4.0|\n",
      "|            2|       4.0|\n",
      "|            1|       4.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            0|       4.0|\n",
      "|            4|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            3|       4.0|\n",
      "|            1|       4.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.245\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       7|  0|  0|  0|  1|\n",
      "|               152880177|  0|  0|  0|  1|\n",
      "|                      15|  0|  0|  0|  1|\n",
      "|                       3| 50|318| 26|563|\n",
      "|                       8|  0|  1|  0|  2|\n",
      "|                      16|  0|  1|  0|  0|\n",
      "|                       0|  9| 47|  5| 55|\n",
      "|                       5|  0|  1|  0|  3|\n",
      "|                       9|  0|  1|  0|  0|\n",
      "|                       1| 51|392| 37|447|\n",
      "|                      10|  0|  1|  0|  0|\n",
      "|                       4| 38|296| 18|844|\n",
      "|                       2| 39|480| 36|682|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = LogisticRegression\n",
    "classifier = LogisticRegression(labelCol='AdoptionSpeed').fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.evaluate(test_data) # Evaluate the model on the validation data\n",
    "results.predictions.show() # Show the predictions\n",
    "results.predictions.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results.predictions)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.predictions.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            1|[3.0,15.0,14.0,10...|[0.06,0.3,0.28,0....|       1.0|\n",
      "|[307.0,0.0,1.0]|            2|[3.0,15.0,14.0,10...|[0.06,0.3,0.28,0....|       1.0|\n",
      "|[307.0,0.0,1.0]|            3|[3.0,15.0,14.0,10...|[0.06,0.3,0.28,0....|       1.0|\n",
      "|[189.0,1.0,1.0]|            1|[8.0,76.0,60.0,82...|[0.03333333333333...|       3.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            0|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[12.0,160.0,275.0...|[0.01522842639593...|       2.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            1|       1.0|\n",
      "|            2|       1.0|\n",
      "|            3|       1.0|\n",
      "|            1|       3.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            2|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            0|       2.0|\n",
      "|            4|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.296\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       7|  0|  0|  0|  1|\n",
      "|               152880177|  0|  0|  0|  1|\n",
      "|                      15|  0|  1|  0|  0|\n",
      "|                       3|234|342| 20|361|\n",
      "|                       8|  1|  1|  0|  1|\n",
      "|                      16|  1|  0|  0|  0|\n",
      "|                       0| 49| 39|  2| 26|\n",
      "|                       5|  1|  0|  0|  3|\n",
      "|                       9|  1|  0|  0|  0|\n",
      "|                       1|350|306| 25|246|\n",
      "|                      10|  0|  1|  0|  0|\n",
      "|                       4|186|311| 10|689|\n",
      "|                       2|371|441| 35|390|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = DecisionTreeClassifier( labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            1|[0.37678324199034...|[0.01883916209951...|       2.0|\n",
      "|[307.0,0.0,1.0]|            2|[0.37678324199034...|[0.01883916209951...|       2.0|\n",
      "|[307.0,0.0,1.0]|            3|[0.37678324199034...|[0.01883916209951...|       2.0|\n",
      "|[189.0,1.0,1.0]|            1|[0.72672189039426...|[0.03633609451971...|       1.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            0|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[0.35554314734002...|[0.01777715736700...|       2.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       1.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            2|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            0|       2.0|\n",
      "|            4|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.298\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       7|  0|  0|  0|  1|\n",
      "|               152880177|  0|  0|  0|  1|\n",
      "|                      15|  0|  1|  0|  0|\n",
      "|                       3|237|302| 30|388|\n",
      "|                       8|  1|  1|  0|  1|\n",
      "|                      16|  1|  0|  0|  0|\n",
      "|                       0| 50| 29|  4| 33|\n",
      "|                       5|  1|  0|  0|  3|\n",
      "|                       9|  1|  0|  0|  0|\n",
      "|                       1|354|276| 32|265|\n",
      "|                      10|  0|  0|  0|  1|\n",
      "|                       4|178|277| 16|725|\n",
      "|                       2|365|401| 47|424|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = RandomForestClassifier(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            1|[-22.691842294142...|[0.00400736974611...|       2.0|\n",
      "|[307.0,0.0,1.0]|            2|[-22.691842294142...|[0.00400736974611...|       2.0|\n",
      "|[307.0,0.0,1.0]|            3|[-22.691842294142...|[0.00400736974611...|       2.0|\n",
      "|[189.0,1.0,1.0]|            1|[-20.564890174319...|[0.01414838951026...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            0|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[-25.948378005932...|[0.00577167179646...|       2.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            2|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            0|       2.0|\n",
      "|            4|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.201\n",
      "+------------------------+---+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|2.0|3.0|4.0|7.0|8.0|\n",
      "+------------------------+---+---+---+---+---+\n",
      "|                       7|  0|  1|  0|  0|  0|\n",
      "|               152880177|  1|  0|  0|  0|  0|\n",
      "|                      15|  1|  0|  0|  0|  0|\n",
      "|                       3|713| 28|114|  7| 95|\n",
      "|                       8|  3|  0|  0|  0|  0|\n",
      "|                      16|  1|  0|  0|  0|  0|\n",
      "|                       0| 79|  3| 16|  0| 18|\n",
      "|                       5|  2|  1|  0|  0|  1|\n",
      "|                       9|  1|  0|  0|  0|  0|\n",
      "|                       1|727|  6|100|  9| 85|\n",
      "|                      10|  0|  0|  1|  0|  0|\n",
      "|                       4|754| 49|229| 33|131|\n",
      "|                       2|975| 16|123|  8|115|\n",
      "+------------------------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = NaiveBayes(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
