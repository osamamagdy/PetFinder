{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting the libraries\n",
    "import pyspark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 3 features:  'Type', 'Age', 'Breed1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read the data and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data:  10540\n",
      "Size of the test data:  4453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To work with spark we need to create a spark session\n",
    "# Need to instal java\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local=[*]\").appName('petfinder').getOrCreate()\n",
    "\n",
    "# Importing the dataset and split it into training and test data\n",
    "df_spark, df_spark_test = spark.read.csv('./train.csv', header=True, inferSchema=True).randomSplit([0.7, 0.3])\n",
    "\n",
    "######### For the training data #########\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark = df_spark.withColumn(\"AdoptionSpeed\", df_spark[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "# Select the columns that we need ['Type', 'Age', 'Breed1']\n",
    "df_spark = df_spark.select(['Type', 'Age', 'Breed1', 'AdoptionSpeed'])\n",
    "\n",
    "######### For the test data #########\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark_test = df_spark_test.withColumn(\"AdoptionSpeed\", df_spark_test[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "# Select the columns that we need ['Type', 'Age', 'Breed1']\n",
    "df_spark_test = df_spark_test.select(['Type', 'Age', 'Breed1', 'AdoptionSpeed'])\n",
    "\n",
    "\n",
    "\n",
    "## Print size of the data\n",
    "print(\"Size of the training data: \", df_spark.count())\n",
    "print(\"Size of the test data: \", df_spark_test.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+-------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|\n",
      "+----+---+------+-------------+\n",
      "|   1|  0|   307|            1|\n",
      "|   1|  0|   307|            4|\n",
      "|   1|  0|   307|            2|\n",
      "|   1|  0|   307|            4|\n",
      "|   1|  0|   307|            1|\n",
      "|   1|  0|   307|            0|\n",
      "|   1|  0|   307|            3|\n",
      "|   1|  0|   307|            4|\n",
      "|   1|  1|   141|            4|\n",
      "|   1|  1|   173|            0|\n",
      "|   1|  1|   189|            1|\n",
      "|   1|  1|   189|            0|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            1|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            3|\n",
      "+----+---+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---+------+-------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|\n",
      "+----+---+------+-------------+\n",
      "|   1|  0|   307|            2|\n",
      "|   1|  1|   128|            3|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            3|\n",
      "|   1|  1|   307|            1|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            1|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            2|\n",
      "|   1|  1|   307|            4|\n",
      "|   1|  1|   307|            1|\n",
      "|   1|  1|   307|            2|\n",
      "+----+---+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### For the train dataset\n",
    "\n",
    "## Drop rows with missing values\n",
    "# df_spark.na.drop(how='all', thresh=10).show() \n",
    "    ### how='any' means drop rows with any missing value, how='all' means drop rows whose all values are missing\n",
    "    ### thresh=10 means drop rows whose number of missing values is greater than 10\n",
    "    ### subset=['Age'] means drop rows whose 'Age' value is missing\n",
    "df_spark = df_spark.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "## Fill missing values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean_val = df_spark.select(mean(df_spark['Age'])).collect()\n",
    "mean_age = mean_val[0][0]\n",
    "df_spark.na.fill(mean_age, subset=['Age']).show()\n",
    "\n",
    "########### For the test dataset\n",
    "\n",
    "df_spark_test = df_spark_test.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "## Fill missing values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean_val = df_spark_test.select(mean(df_spark_test['Age'])).collect()\n",
    "mean_age = mean_val[0][0]\n",
    "df_spark_test.na.fill(mean_age, subset=['Age']).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark MLlib to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+-------------+---------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|       features|\n",
      "+----+---+------+-------------+---------------+\n",
      "|   1|  0|   307|            1|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            4|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            2|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            4|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            1|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            0|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            3|[307.0,0.0,1.0]|\n",
      "|   1|  0|   307|            4|[307.0,0.0,1.0]|\n",
      "|   1|  1|   141|            4|[141.0,1.0,1.0]|\n",
      "|   1|  1|   173|            0|[173.0,1.0,1.0]|\n",
      "|   1|  1|   189|            1|[189.0,1.0,1.0]|\n",
      "|   1|  1|   189|            0|[189.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "+----+---+------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---+------+-------------+---------------+\n",
      "|Type|Age|Breed1|AdoptionSpeed|       features|\n",
      "+----+---+------+-------------+---------------+\n",
      "|   1|  0|   307|            2|[307.0,0.0,1.0]|\n",
      "|   1|  1|   128|            3|[128.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            3|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            4|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            1|[307.0,1.0,1.0]|\n",
      "|   1|  1|   307|            2|[307.0,1.0,1.0]|\n",
      "+----+---+------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, collect the features in a single column\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#### For the train dataset\n",
    "featureassemble = VectorAssembler(inputCols=['Breed1','Age','Type'], outputCol='features')\n",
    "output = featureassemble.transform(df_spark) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "output.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfeatureassemble = VectorAssembler(inputCols=['Breed1','Age','Type'], outputCol='features')\n",
    "testoutput = testfeatureassemble.transform(df_spark_test) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "testoutput.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|       features|AdoptionSpeed|\n",
      "+---------------+-------------+\n",
      "|[307.0,0.0,1.0]|            1|\n",
      "|[307.0,0.0,1.0]|            4|\n",
      "|[307.0,0.0,1.0]|            2|\n",
      "|[307.0,0.0,1.0]|            4|\n",
      "|[307.0,0.0,1.0]|            1|\n",
      "|[307.0,0.0,1.0]|            0|\n",
      "|[307.0,0.0,1.0]|            3|\n",
      "|[307.0,0.0,1.0]|            4|\n",
      "|[141.0,1.0,1.0]|            4|\n",
      "|[173.0,1.0,1.0]|            0|\n",
      "|[189.0,1.0,1.0]|            1|\n",
      "|[189.0,1.0,1.0]|            0|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-------------+\n",
      "|       features|AdoptionSpeed|\n",
      "+---------------+-------------+\n",
      "|[307.0,0.0,1.0]|            2|\n",
      "|[128.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            3|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "|[307.0,1.0,1.0]|            4|\n",
      "|[307.0,1.0,1.0]|            1|\n",
      "|[307.0,1.0,1.0]|            2|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the features and the target column\n",
    "\n",
    "#### For the train dataset\n",
    "finalized_data = output.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "finalized_data.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfinalized_data = testoutput.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "testfinalized_data.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            2|[3.16169972978996...|[0.01709492317757...|       2.0|\n",
      "|[128.0,1.0,1.0]|            3|[4.01073097781256...|[0.03253082360487...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            4|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            3|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            3|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            3|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            1|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            4|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            1|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            4|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            1|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "|[307.0,1.0,1.0]|            2|[3.18020601443288...|[0.01707240028563...|       4.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            3|       4.0|\n",
      "|            3|       4.0|\n",
      "|            3|       4.0|\n",
      "|            1|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            1|       4.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            2|       4.0|\n",
      "|            4|       4.0|\n",
      "|            1|       4.0|\n",
      "|            2|       4.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.247\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       7|  0|  0|  0|  1|\n",
      "|               152880177|  0|  0|  0|  1|\n",
      "|                       3| 35|362| 23|512|\n",
      "|                       8|  0|  1|  0|  2|\n",
      "|                      16|  0|  1|  0|  0|\n",
      "|                       0|  8| 54|  4| 46|\n",
      "|                       5|  0|  2|  0|  0|\n",
      "|                       6|  1|  0|  0|  0|\n",
      "|                       1| 46|455| 29|383|\n",
      "|                       4| 28|320| 19|868|\n",
      "|                       2| 39|459| 20|657|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = LogisticRegression\n",
    "classifier = LogisticRegression(labelCol='AdoptionSpeed').fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.evaluate(test_data) # Evaluate the model on the validation data\n",
    "results.predictions.show() # Show the predictions\n",
    "results.predictions.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results.predictions)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.predictions.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[128.0,1.0,1.0]|            3|[2.0,30.0,26.0,19...|[0.02352941176470...|       1.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[41.0,427.0,693.0...|[0.01959847036328...|       2.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            2|       2.0|\n",
      "|            3|       1.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.313\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       7|  0|  0|  0|  1|\n",
      "|               152880177|  0|  0|  0|  1|\n",
      "|                       3|109|485| 60|278|\n",
      "|                       8|  0|  2|  0|  1|\n",
      "|                      16|  0|  1|  0|  0|\n",
      "|                       0| 24| 46| 13| 29|\n",
      "|                       5|  0|  1|  0|  1|\n",
      "|                       6|  1|  0|  0|  0|\n",
      "|                       1|149|511| 61|192|\n",
      "|                       4| 88|453| 40|654|\n",
      "|                       2|129|686| 57|303|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = DecisionTreeClassifier( labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            2|[0.43416348295369...|[0.02170817414768...|       2.0|\n",
      "|[128.0,1.0,1.0]|            3|[0.72443176663505...|[0.03622158833175...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[0.35548404163702...|[0.01777420208185...|       2.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.306\n",
      "+------------------------+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+\n",
      "|                       7|  0|  0|  0|  1|\n",
      "|               152880177|  0|  0|  0|  1|\n",
      "|                       3|115|491| 20|306|\n",
      "|                       8|  0|  3|  0|  0|\n",
      "|                      16|  0|  1|  0|  0|\n",
      "|                       0| 34| 45|  1| 32|\n",
      "|                       5|  0|  1|  0|  1|\n",
      "|                       6|  0|  1|  0|  0|\n",
      "|                       1|181|507| 14|211|\n",
      "|                       4|113|426|  3|693|\n",
      "|                       2|140|673| 19|343|\n",
      "+------------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = RandomForestClassifier(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|       features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "|[307.0,0.0,1.0]|            2|[-22.437679169332...|[0.00282065235772...|      13.0|\n",
      "|[128.0,1.0,1.0]|            3|[-17.697206668521...|[0.02210479811797...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            3|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            4|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            1|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "|[307.0,1.0,1.0]|            2|[-25.713613854880...|[0.00581037364618...|       2.0|\n",
      "+---------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            2|      13.0|\n",
      "|            3|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            3|       2.0|\n",
      "|            1|       2.0|\n",
      "|            4|       2.0|\n",
      "|            2|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            2|       2.0|\n",
      "|            4|       2.0|\n",
      "|            1|       2.0|\n",
      "|            2|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.188\n",
      "+------------------------+----+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|13.0|2.0|3.0|4.0|7.0|\n",
      "+------------------------+----+---+---+---+---+\n",
      "|                       7|   0|  0|  1|  0|  0|\n",
      "|               152880177|   0|  1|  0|  0|  0|\n",
      "|                       3|   3|698| 14|122| 95|\n",
      "|                       8|   0|  2|  0|  0|  1|\n",
      "|                      16|   0|  1|  0|  0|  0|\n",
      "|                       0|   1| 70|  4| 21| 16|\n",
      "|                       5|   0|  2|  0|  0|  0|\n",
      "|                       6|   0|  1|  0|  0|  0|\n",
      "|                       1|   6|710| 11| 92| 94|\n",
      "|                       4|   4|756| 64|228|183|\n",
      "|                       2|   7|904| 23|130|111|\n",
      "+------------------------+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = NaiveBayes(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
