{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4oKPRtc3np5"
      },
      "outputs": [],
      "source": [
        "# ! pip install pyspark\n",
        "# ! pip install spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frequent Items and Association Rules with normal python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvjDCjQq3hgn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Load the PetFinder dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Define the columns for analysis\n",
        "columns = [\"Type\", \"Age\", \"Breed1\", \"Gender\", \"Color1\", \"Color2\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\"]\n",
        "\n",
        "# Select the columns for analysis\n",
        "data = df[columns]\n",
        "\n",
        "# Convert the data to a list of lists for Apriori input\n",
        "transactions = data.values.tolist()\n",
        "\n",
        "# Apply the Apriori algorithm to get the frequent itemsets\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "freqItemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
        "\n",
        "# Apply the association rules algorithm to get the association rules\n",
        "rules = association_rules(freqItemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "\n",
        "# save the results to a csv file\n",
        "freqItemsets.to_csv(\"freqItemsets.csv\")\n",
        "rules.to_csv(\"rules.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frequent Items with Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9CyDVU-awXH"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.fpm import FPGrowth\n",
        "# from pyspark.mllib.fpm import AssociationRules\n",
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "# Load the PetFinder dataset into a Spark DataFrame\n",
        "df = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Define the columns for analysis, all columns except the non-numerical columns\n",
        "columns = [\"Age\", \"Breed1\", \"Color1\", \"Color2\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\"]\n",
        "\n",
        "# Select the columns for analysis\n",
        "data = df.select(columns)\n",
        "\n",
        "# Convert the data to a list of lists for FPGrowth input\n",
        "transactions = data.rdd.map(lambda x: x).collect()\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf())\n",
        "\n",
        "# Convert the transactions to an RDD of itemsets\n",
        "rdd = sc.parallelize(transactions).map(lambda x: set(x))\n",
        "\n",
        "# Train the FPGrowth model to get the frequent itemsets\n",
        "model = FPGrowth.train(rdd, minSupport=0.2, numPartitions=10)\n",
        "\n",
        "# Get the frequent itemsets and save them to a file\n",
        "freqItems = model.freqItemsets().collect()\n",
        "with open(\"freqItemsets.txt\", \"a\") as f:\n",
        "    for fi in freqItems:\n",
        "        f.write(str(fi) + \"\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM1miQtii8Gq",
        "outputId": "3c2be2e8-b22a-4ac3-decd-e436e241f426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----+\n",
            "|    items|freq|\n",
            "+---------+----+\n",
            "|      [5]|   2|\n",
            "|   [5, 1]|   2|\n",
            "|[5, 1, 2]|   2|\n",
            "|   [5, 2]|   2|\n",
            "|      [2]|   3|\n",
            "|      [1]|   3|\n",
            "|   [1, 2]|   3|\n",
            "+---------+----+\n",
            "\n",
            "+----------+----------+------------------+----+------------------+\n",
            "|antecedent|consequent|        confidence|lift|           support|\n",
            "+----------+----------+------------------+----+------------------+\n",
            "|       [5]|       [1]|               1.0| 1.0|0.6666666666666666|\n",
            "|       [5]|       [2]|               1.0| 1.0|0.6666666666666666|\n",
            "|    [1, 2]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
            "|    [5, 2]|       [1]|               1.0| 1.0|0.6666666666666666|\n",
            "|    [5, 1]|       [2]|               1.0| 1.0|0.6666666666666666|\n",
            "|       [2]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
            "|       [2]|       [1]|               1.0| 1.0|               1.0|\n",
            "|       [1]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
            "|       [1]|       [2]|               1.0| 1.0|               1.0|\n",
            "+----------+----------+------------------+----+------------------+\n",
            "\n",
            "+---+------------+----------+\n",
            "| id|       items|prediction|\n",
            "+---+------------+----------+\n",
            "|  0|   [1, 2, 5]|        []|\n",
            "|  1|[1, 2, 3, 5]|        []|\n",
            "|  2|      [1, 2]|       [5]|\n",
            "+---+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (0, [1, 2, 5]),\n",
        "    (1, [1, 2, 3, 5]),\n",
        "    (2, [1, 2])\n",
        "], [\"id\", \"items\"])\n",
        "\n",
        "# save the df in txt file  \n",
        "with open(\"df.txt\", \"w\") as f:\n",
        "    for row in df:\n",
        "        f.write(str(row) + \"\\n\")\n",
        "\n",
        "\n",
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
        "model = fpGrowth.fit(df)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "\n",
        "# transform examines the input items against all the association rules and summarize the\n",
        "# consequents as prediction\n",
        "model.transform(df).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract useful insights from Accosiation Rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGKfJ29EvnJi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
