{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install pyspark\n",
        "# ! pip install spark"
      ],
      "metadata": {
        "id": "G4oKPRtc3np5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430f35ad-f13c-4ba2-c395-271ca91351ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=815f7e4767efbe1cec425b38ace45ccd0b19043c92e722379441f4ec119b2836\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequent Items and Association Rules with normal python"
      ],
      "metadata": {
        "id": "xzUTQse4iXQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pvjDCjQq3hgn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Load the PetFinder dataset into a Pandas DataFrame\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Define the columns for analysis\n",
        "columns = [\"Type\", \"Age\", \"Breed1\", \"Gender\", \"Color1\", \"Color2\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"Quantity\", \"Fee\", \"State\"]\n",
        "\n",
        "# Select the columns for analysis\n",
        "data = df[columns]\n",
        "\n",
        "# Convert the data to a list of lists for Apriori input\n",
        "transactions = data.values.tolist()\n",
        "\n",
        "# Apply the Apriori algorithm to get the frequent itemsets\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "freqItemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Apply the association rules algorithm to get the association rules\n",
        "rules = association_rules(freqItemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "\n",
        "# save the results to a csv file\n",
        "freqItemsets.to_csv(\"freqItems.csv\")\n",
        "rules.to_csv(\"rules.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequent Items, AssociationRules with Spark"
      ],
      "metadata": {
        "id": "Mw494xjnikmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Map the values from numeric to the meaningful non-numeric values"
      ],
      "metadata": {
        "id": "RV016PKaiyQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map values in the df to it's actual non-numeric value \n",
        "def map_values(df):\n",
        "    # create new df\n",
        "    new_df = pd.DataFrame(columns=df.columns, index=df.index)\n",
        "    # create map dict\n",
        "    map_dict = {\n",
        "        \"Type\": {\n",
        "            1: \"Dog\",\n",
        "            2: \"Cat\"\n",
        "        },\n",
        "        \"Color1\": {\n",
        "            1: \"Black\",\n",
        "            2: \"Brown\",\n",
        "            3: \"Golden\",\n",
        "            4: \"Yellow\",\n",
        "            5: \"Cream\",\n",
        "            6: \"Gray\",\n",
        "            7: \"White\"\n",
        "        },\n",
        "        \"MaturitySize\": {\n",
        "            1: \"Small MaturitySize\",\n",
        "            2: \"Medium MaturitySize\",\n",
        "            3: \"Large MaturitySize\",\n",
        "            4: \"Extra Large MaturitySize\",\n",
        "            0: \"Not Specified MaturitySize\"\n",
        "        },\n",
        "        \"FurLength\": {\n",
        "            1: \"Short Fur\",\n",
        "            2: \"Medium Fur\",\n",
        "            3: \"Long Fur\",\n",
        "            0: \"Not Specified Fur\"\n",
        "        },\n",
        "        \"Vaccinated\": {\n",
        "            1: \"Yes Vaccinated\",\n",
        "            2: \"No Vaccinated\",\n",
        "            3: \"Not Sure Vaccinated\"\n",
        "        },\n",
        "        \"Dewormed\": {\n",
        "            1: \"Yes Dewormed\",\n",
        "            2: \"No Dewormed\",\n",
        "            3: \"Not Sure Dewormed\"\n",
        "        },\n",
        "        \"Sterilized\": {\n",
        "            1: \"Yes Sterilized\",\n",
        "            2: \"No Sterilized\",\n",
        "            3: \"Not Sure Sterilized\"\n",
        "        },\n",
        "        \"Health\": {\n",
        "            1: \"Healthy\",\n",
        "            2: \"Minor Injury\",\n",
        "            3: \"Serious Injury\",\n",
        "            0: \"Not Specified Health\"\n",
        "        },\n",
        "        \"AdoptionSpeed\": {\n",
        "            \"0\": \"Adoption on the same day\",\n",
        "            \"1\": \"Adoption in the first week\",\n",
        "            \"2\": \"Adoption in the first month\",\n",
        "            \"3\": \"Adoption within the first 3 months\",\n",
        "            \"4\": \"No adoption after 100 days\"\n",
        "        }\n",
        "    }\n",
        "    for i in range (0, len(df.columns)):\n",
        "        new_df.iloc[:,i] = df.iloc[:,i].map(map_dict[df.columns[i]])    \n",
        "    return new_df\n"
      ],
      "metadata": {
        "id": "0oJBzNHGTU5h"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare the dataset"
      ],
      "metadata": {
        "id": "deJdXtybjGiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Insights\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Define the columns for analysis, all columns except the non-numerical columns\n",
        "columns = [\"Type\", \"Color1\", \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\",\"AdoptionSpeed\"]\n",
        "\n",
        "# Convert the df to be a list of lists\n",
        "df = df.select(columns).toPandas()\n",
        "\n",
        "df = map_values(df)\n",
        "\n",
        "df = df.values.tolist()\n",
        "\n",
        "# add id to the df \n",
        "for i in range(len(df)):\n",
        "    df[i] = [i , df[i]]\n",
        "\n",
        "# save the df to a file\n",
        "# with open(\"df.txt\", \"w\") as f:\n",
        "#     for row in df:\n",
        "#         f.write(str(row) + \"\\n\")\n",
        "\n",
        "# convert df to spark dataframe with id and items columns \n",
        "df = spark.createDataFrame(df, [\"id\", \"items\"])\n",
        "\n",
        "# save the df to a file\n",
        "# with open(\"df_spark.txt\", \"w\") as f:\n",
        "#     for row in df:\n",
        "#         f.write(str(row) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "8rBgKANthsBD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract frequect itemsets and association rules"
      ],
      "metadata": {
        "id": "o69BXe2SjgYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
        "model = fpGrowth.fit(df)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "\n",
        "# save the frequent itemsets to a file\n",
        "# with open(\"freqItemsetsFinal.txt\", \"w\") as f:\n",
        "#     for row in model.freqItemsets.collect():\n",
        "#         f.write(str(row) + \"\\n\")\n",
        "\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "\n",
        "# save the association rules to a file\n",
        "# with open(\"associationRulesFinal.txt\", \"w\") as f:\n",
        "#     for row in model.associationRules.collect():\n",
        "#         f.write(str(row) + \"\\n\")\n",
        "\n",
        "# transform examines the input items against all the association rules\n",
        "# then summarize the consequents as prediction\n",
        "model.transform(df).show()\n",
        "\n",
        "# save the predictions to a file\n",
        "# with open(\"predictionsFinal.txt\", \"w\") as f:\n",
        "#     for row in model.transform(df).collect():\n",
        "#         f.write(str(row) + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgCG4LCGBx2D",
        "outputId": "18942d8f-cf55-4fdc-a432-ca399e317aef"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|               items| freq|\n",
            "+--------------------+-----+\n",
            "|     [No Sterilized]|10077|\n",
            "|[No Sterilized, H...| 9782|\n",
            "|      [Yes Dewormed]| 8397|\n",
            "|[Yes Dewormed, He...| 8161|\n",
            "|           [Healthy]|14478|\n",
            "|               [Dog]| 8132|\n",
            "|      [Dog, Healthy]| 7845|\n",
            "|[Medium MaturityS...|10305|\n",
            "|[Medium MaturityS...|10030|\n",
            "|         [Short Fur]| 8808|\n",
            "|[Short Fur, Healthy]| 8536|\n",
            "+--------------------+-----+\n",
            "\n",
            "+--------------------+--------------------+------------------+------------------+------------------+\n",
            "|          antecedent|          consequent|        confidence|              lift|           support|\n",
            "+--------------------+--------------------+------------------+------------------+------------------+\n",
            "|     [No Sterilized]|           [Healthy]|0.9707254143098144|1.0052552933241503|0.6524378043086774|\n",
            "|         [Short Fur]|           [Healthy]|0.9691189827429609|1.0035917190402828|0.5693323550990462|\n",
            "|      [Yes Dewormed]|           [Healthy]|0.9718947243062999| 1.006466197093822|0.5443206829853932|\n",
            "|           [Healthy]|     [No Sterilized]|0.6756458074319658|1.0052552933241503|0.6524378043086774|\n",
            "|           [Healthy]|[Medium MaturityS...|0.6927752451996132| 1.007935880764464|0.6689788567998399|\n",
            "|               [Dog]|           [Healthy]|0.9647073290703394|0.9990231375018371|0.5232441806176216|\n",
            "|[Medium MaturityS...|           [Healthy]|0.9733139252789907|1.0079358807644638|0.6689788567998399|\n",
            "+--------------------+--------------------+------------------+------------------+------------------+\n",
            "\n",
            "+---+--------------------+--------------------+\n",
            "| id|               items|          prediction|\n",
            "+---+--------------------+--------------------+\n",
            "|  0|[Cat, Black, Smal...|[Medium MaturityS...|\n",
            "|  1|[Cat, Black, Medi...|     [No Sterilized]|\n",
            "|  2|[Dog, Brown, Medi...|                  []|\n",
            "|  3|[Dog, Black, Medi...|                  []|\n",
            "|  4|[Dog, Black, Medi...|                  []|\n",
            "|  5|[Cat, Cream, Medi...|                  []|\n",
            "|  6|[Cat, Black, Medi...|     [No Sterilized]|\n",
            "|  7|[Dog, Black, Medi...|                  []|\n",
            "|  8|[Cat, Gray, Mediu...|                  []|\n",
            "|  9|[Cat, Black, Medi...|     [No Sterilized]|\n",
            "| 10|[Dog, Black, Medi...|                  []|\n",
            "| 11|[Cat, Black, Larg...|[Medium MaturityS...|\n",
            "| 12|[Dog, Brown, Medi...|                  []|\n",
            "| 13|[Cat, Black, Smal...|[No Sterilized, M...|\n",
            "| 14|[Dog, Brown, Medi...|     [No Sterilized]|\n",
            "| 15|[Dog, Black, Medi...|     [No Sterilized]|\n",
            "| 16|[Cat, Brown, Smal...|[No Sterilized, M...|\n",
            "| 17|[Dog, Brown, Medi...|     [No Sterilized]|\n",
            "| 18|[Dog, Black, Medi...|                  []|\n",
            "| 19|[Cat, Black, Smal...|[Medium MaturityS...|\n",
            "+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGKfJ29EvnJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}