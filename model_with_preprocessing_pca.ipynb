{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting the libraries\n",
    "import pyspark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read the data and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/06 11:16:50 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.17.130 instead (on interface ens33)\n",
      "23/05/06 11:16:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/06 11:16:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/06 11:16:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data:  16870\n",
      "Size of the test data:  2999\n"
     ]
    }
   ],
   "source": [
    "# To work with spark we need to create a spark session\n",
    "# Need to instal java\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('petfinder').getOrCreate()\n",
    "\n",
    "########### For the train dataset\n",
    "# Read a dataset with spark\n",
    "df_spark = spark.read.csv('./train_balanced_corr_pca.csv', header=True, inferSchema=True)\n",
    "# Header = True, inferSchema = True, means that the first row is the header and the schema is inferred (if schema is not inferred, all columns will be read as string)\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark = df_spark.withColumn(\"AdoptionSpeed\", df_spark[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "\n",
    "\n",
    "############ For the test dataset\n",
    "# Read a dataset with spark\n",
    "df_spark_test = spark.read.csv('./test_split_corr_pca.csv', header=True, inferSchema=True)\n",
    "# Header = True, inferSchema = True, means that the first row is the header and the schema is inferred (if schema is not inferred, all columns will be read as string)\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark_test = df_spark_test.withColumn(\"AdoptionSpeed\", df_spark_test[\"AdoptionSpeed\"].cast(\"integer\"))\n",
    "\n",
    "\n",
    "## Print size of the data\n",
    "print(\"Size of the training data: \", df_spark.count())\n",
    "print(\"Size of the test data: \", df_spark_test.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+\n",
      "|               PC 1|               PC 2|AdoptionSpeed|\n",
      "+-------------------+-------------------+-------------+\n",
      "| -45.79889769398241|-3.7878222930805943|            1|\n",
      "|-42.343623530864804| 30.035857366316367|            4|\n",
      "|  81.53834585803112| -16.79632262357971|            1|\n",
      "|-1.4767092940335453| 16.810059608539753|            4|\n",
      "| -45.79889769398213|-3.7878222930861187|            1|\n",
      "|-2.2682800879238054|  18.90131673263072|            4|\n",
      "|  180.4965526851584|  7.270981787952771|            1|\n",
      "| -43.56313205902385| 18.098088074762675|            3|\n",
      "|-43.868009191063614|   15.1136457518742|            4|\n",
      "|-43.055003505624235| 23.072158612910123|            3|\n",
      "| -45.69727198330221| -2.793008185456628|            2|\n",
      "| 211.91402333302426|  88.49790192569439|            2|\n",
      "|  116.9883125474134| -4.334679801795222|            2|\n",
      "|  9.161457941677828|  12.70769928849949|            3|\n",
      "|  59.12824242464578|  19.66970866545448|            2|\n",
      "| -45.79889769398213|-3.7878222930861187|            3|\n",
      "|-3.9959171694824804|  1.989476902929381|            3|\n",
      "| -45.69727198330221| -2.793008185456628|            2|\n",
      "|-17.249369244257043| 39.530476227734724|            4|\n",
      "|-3.9157263503516746|-7.0654789745680135|            2|\n",
      "+-------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+-------------+\n",
      "|                PC 1|               PC 2|AdoptionSpeed|\n",
      "+--------------------+-------------------+-------------+\n",
      "|-0.46468319187692225| -3.239589737740624|            4|\n",
      "|   0.187818769725258| 13.777139802157686|            4|\n",
      "| -0.9753292896775865| 1.8337523415074812|            2|\n",
      "|   74.34352372310829|  42.72942431196087|            1|\n",
      "|   -1.84769033422972|  -7.12378825398028|            4|\n",
      "|  -41.78167424033198|  5.812105848113322|            2|\n",
      "|  47.186951686600864| 1.1664082810465353|            4|\n",
      "| -18.722712558763572| 15.618821911053569|            3|\n",
      "|   1.350966829128103| 25.720527262808176|            4|\n",
      "|  155.89354312772377|-17.458220130186945|            2|\n",
      "|  -39.45537812152629|  29.69888076941402|            3|\n",
      "|  -41.49088722548126|   8.79795271327591|            4|\n",
      "|  248.51947016971596|  25.76656278265796|            2|\n",
      "|  -41.20010021063055| 11.783799578438497|            3|\n",
      "| -42.847893294784576|-5.1359993241494974|            1|\n",
      "|   17.61133866366241|  48.25027326176463|            3|\n",
      "| -20.467434647867833|-2.2962592799219492|            2|\n",
      "|   175.2177915490632|-25.368526606718163|            3|\n",
      "|   -42.6540352848841| -3.145434747374438|            1|\n",
      "| -42.847893294784576|-5.1359993241494974|            1|\n",
      "+--------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### For the train dataset\n",
    "\n",
    "## Drop rows with missing values\n",
    "df_spark = df_spark.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "df_spark.show()\n",
    "########### For the test dataset\n",
    "df_spark_test = df_spark_test.na.drop(how= 'any' , subset=['AdoptionSpeed'])\n",
    "df_spark_test.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark MLlib to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+--------------------+\n",
      "|               PC 1|               PC 2|AdoptionSpeed|            features|\n",
      "+-------------------+-------------------+-------------+--------------------+\n",
      "| -45.79889769398241|-3.7878222930805943|            1|[-45.798897693982...|\n",
      "|-42.343623530864804| 30.035857366316367|            4|[-42.343623530864...|\n",
      "|  81.53834585803112| -16.79632262357971|            1|[81.5383458580311...|\n",
      "|-1.4767092940335453| 16.810059608539753|            4|[-1.4767092940335...|\n",
      "| -45.79889769398213|-3.7878222930861187|            1|[-45.798897693982...|\n",
      "|-2.2682800879238054|  18.90131673263072|            4|[-2.2682800879238...|\n",
      "|  180.4965526851584|  7.270981787952771|            1|[180.496552685158...|\n",
      "| -43.56313205902385| 18.098088074762675|            3|[-43.563132059023...|\n",
      "|-43.868009191063614|   15.1136457518742|            4|[-43.868009191063...|\n",
      "|-43.055003505624235| 23.072158612910123|            3|[-43.055003505624...|\n",
      "| -45.69727198330221| -2.793008185456628|            2|[-45.697271983302...|\n",
      "| 211.91402333302426|  88.49790192569439|            2|[211.914023333024...|\n",
      "|  116.9883125474134| -4.334679801795222|            2|[116.988312547413...|\n",
      "|  9.161457941677828|  12.70769928849949|            3|[9.16145794167782...|\n",
      "|  59.12824242464578|  19.66970866545448|            2|[59.1282424246457...|\n",
      "| -45.79889769398213|-3.7878222930861187|            3|[-45.798897693982...|\n",
      "|-3.9959171694824804|  1.989476902929381|            3|[-3.9959171694824...|\n",
      "| -45.69727198330221| -2.793008185456628|            2|[-45.697271983302...|\n",
      "|-17.249369244257043| 39.530476227734724|            4|[-17.249369244257...|\n",
      "|-3.9157263503516746|-7.0654789745680135|            2|[-3.9157263503516...|\n",
      "+-------------------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+-------------+--------------------+\n",
      "|                PC 1|               PC 2|AdoptionSpeed|            features|\n",
      "+--------------------+-------------------+-------------+--------------------+\n",
      "|-0.46468319187692225| -3.239589737740624|            4|[-0.4646831918769...|\n",
      "|   0.187818769725258| 13.777139802157686|            4|[0.18781876972525...|\n",
      "| -0.9753292896775865| 1.8337523415074812|            2|[-0.9753292896775...|\n",
      "|   74.34352372310829|  42.72942431196087|            1|[74.3435237231082...|\n",
      "|   -1.84769033422972|  -7.12378825398028|            4|[-1.8476903342297...|\n",
      "|  -41.78167424033198|  5.812105848113322|            2|[-41.781674240331...|\n",
      "|  47.186951686600864| 1.1664082810465353|            4|[47.1869516866008...|\n",
      "| -18.722712558763572| 15.618821911053569|            3|[-18.722712558763...|\n",
      "|   1.350966829128103| 25.720527262808176|            4|[1.35096682912810...|\n",
      "|  155.89354312772377|-17.458220130186945|            2|[155.893543127723...|\n",
      "|  -39.45537812152629|  29.69888076941402|            3|[-39.455378121526...|\n",
      "|  -41.49088722548126|   8.79795271327591|            4|[-41.490887225481...|\n",
      "|  248.51947016971596|  25.76656278265796|            2|[248.519470169715...|\n",
      "|  -41.20010021063055| 11.783799578438497|            3|[-41.200100210630...|\n",
      "| -42.847893294784576|-5.1359993241494974|            1|[-42.847893294784...|\n",
      "|   17.61133866366241|  48.25027326176463|            3|[17.6113386636624...|\n",
      "| -20.467434647867833|-2.2962592799219492|            2|[-20.467434647867...|\n",
      "|   175.2177915490632|-25.368526606718163|            3|[175.217791549063...|\n",
      "|   -42.6540352848841| -3.145434747374438|            1|[-42.654035284884...|\n",
      "| -42.847893294784576|-5.1359993241494974|            1|[-42.847893294784...|\n",
      "+--------------------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, collect the features in a single column\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#### For the train dataset\n",
    "featureassemble = VectorAssembler(inputCols=['PC 1','PC 2'], outputCol='features')\n",
    "output = featureassemble.transform(df_spark) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "output.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfeatureassemble = VectorAssembler(inputCols=['PC 1','PC 2'], outputCol='features')\n",
    "testoutput = testfeatureassemble.transform(df_spark_test) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "testoutput.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|AdoptionSpeed|\n",
      "+--------------------+-------------+\n",
      "|[-45.798897693982...|            1|\n",
      "|[-42.343623530864...|            4|\n",
      "|[81.5383458580311...|            1|\n",
      "|[-1.4767092940335...|            4|\n",
      "|[-45.798897693982...|            1|\n",
      "|[-2.2682800879238...|            4|\n",
      "|[180.496552685158...|            1|\n",
      "|[-43.563132059023...|            3|\n",
      "|[-43.868009191063...|            4|\n",
      "|[-43.055003505624...|            3|\n",
      "|[-45.697271983302...|            2|\n",
      "|[211.914023333024...|            2|\n",
      "|[116.988312547413...|            2|\n",
      "|[9.16145794167782...|            3|\n",
      "|[59.1282424246457...|            2|\n",
      "|[-45.798897693982...|            3|\n",
      "|[-3.9959171694824...|            3|\n",
      "|[-45.697271983302...|            2|\n",
      "|[-17.249369244257...|            4|\n",
      "|[-3.9157263503516...|            2|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------+\n",
      "|            features|AdoptionSpeed|\n",
      "+--------------------+-------------+\n",
      "|[-0.4646831918769...|            4|\n",
      "|[0.18781876972525...|            4|\n",
      "|[-0.9753292896775...|            2|\n",
      "|[74.3435237231082...|            1|\n",
      "|[-1.8476903342297...|            4|\n",
      "|[-41.781674240331...|            2|\n",
      "|[47.1869516866008...|            4|\n",
      "|[-18.722712558763...|            3|\n",
      "|[1.35096682912810...|            4|\n",
      "|[155.893543127723...|            2|\n",
      "|[-39.455378121526...|            3|\n",
      "|[-41.490887225481...|            4|\n",
      "|[248.519470169715...|            2|\n",
      "|[-41.200100210630...|            3|\n",
      "|[-42.847893294784...|            1|\n",
      "|[17.6113386636624...|            3|\n",
      "|[-20.467434647867...|            2|\n",
      "|[175.217791549063...|            3|\n",
      "|[-42.654035284884...|            1|\n",
      "|[-42.847893294784...|            1|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the features and the target column\n",
    "\n",
    "#### For the train dataset\n",
    "finalized_data = output.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "finalized_data.show()\n",
    "\n",
    "#### For the test dataset\n",
    "testfinalized_data = testoutput.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "testfinalized_data.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/06 11:17:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-0.4646831918769...|            4|[0.01081458698630...|[0.20194167201004...|       1.0|\n",
      "|[0.18781876972525...|            4|[-0.0338958779557...|[0.19234253063304...|       4.0|\n",
      "|[-0.9753292896775...|            2|[-0.0049339692493...|[0.19899426650005...|       3.0|\n",
      "|[74.3435237231082...|            1|[0.14056653631653...|[0.22533761111126...|       4.0|\n",
      "|[-1.8476903342297...|            4|[0.01678746228048...|[0.20278560206824...|       1.0|\n",
      "|[-41.781674240331...|            2|[-0.1558683876163...|[0.16933260130768...|       4.0|\n",
      "|[47.1869516866008...|            4|[0.16209503325697...|[0.23299498891322...|       0.0|\n",
      "|[-18.722712558763...|            3|[-0.1038366691737...|[0.17780388978986...|       4.0|\n",
      "|[1.35096682912810...|            4|[-0.0628577866622...|[0.18390544908564...|       4.0|\n",
      "|[155.893543127723...|            2|[0.58632334260393...|[0.31211430936379...|       0.0|\n",
      "|[-39.455378121526...|            3|[-0.2137922050291...|[0.15186373912967...|       4.0|\n",
      "|[-41.490887225481...|            4|[-0.1631088647929...|[0.16747716770427...|       4.0|\n",
      "|[248.519470169715...|            2|[0.78475803437683...|[0.38306655087351...|       0.0|\n",
      "|[-41.200100210630...|            3|[-0.1703493419695...|[0.16552287278993...|       4.0|\n",
      "|[-42.847893294784...|            1|[-0.1293199713020...|[0.17526528023611...|       4.0|\n",
      "|[17.6113386636624...|            3|[-0.0692462842302...|[0.17419424387654...|       4.0|\n",
      "|[-20.467434647867...|            2|[-0.0603938061140...|[0.18816933926066...|       2.0|\n",
      "|[175.217791549063...|            3|[0.67442618244976...|[0.32335372872988...|       0.0|\n",
      "|[-42.654035284884...|            1|[-0.1341469560864...|[0.17428984127372...|       4.0|\n",
      "|[-42.847893294784...|            1|[-0.1293199713020...|[0.17526528023611...|       4.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       1.0|\n",
      "|            4|       4.0|\n",
      "|            2|       3.0|\n",
      "|            1|       4.0|\n",
      "|            4|       1.0|\n",
      "|            2|       4.0|\n",
      "|            4|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            1|       4.0|\n",
      "|            3|       4.0|\n",
      "|            2|       2.0|\n",
      "|            3|       0.0|\n",
      "|            1|       4.0|\n",
      "|            1|       4.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.218\n",
      "+------------------------+---+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+---+\n",
      "|                       3|109|180| 24| 14|315|\n",
      "|                       0| 10| 25| 11|  4| 31|\n",
      "|                       1|105|233| 64| 11|206|\n",
      "|                       4| 71|171| 30| 50|501|\n",
      "|                       2| 92|278| 43| 26|395|\n",
      "+------------------------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = LogisticRegression\n",
    "classifier = LogisticRegression(labelCol='AdoptionSpeed').fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.evaluate(test_data) # Evaluate the model on the validation data\n",
    "results.predictions.show() # Show the predictions\n",
    "results.predictions.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results.predictions)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.predictions.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-0.4646831918769...|            4|[118.0,52.0,47.0,...|[0.37942122186495...|       0.0|\n",
      "|[0.18781876972525...|            4|[285.0,225.0,235....|[0.18566775244299...|       4.0|\n",
      "|[-0.9753292896775...|            2|[285.0,225.0,235....|[0.18566775244299...|       4.0|\n",
      "|[74.3435237231082...|            1|[315.0,177.0,136....|[0.35512965050732...|       0.0|\n",
      "|[-1.8476903342297...|            4|[806.0,738.0,535....|[0.28380281690140...|       0.0|\n",
      "|[-41.781674240331...|            2|[36.0,2.0,9.0,8.0...|[0.52173913043478...|       0.0|\n",
      "|[47.1869516866008...|            4|[285.0,225.0,235....|[0.18566775244299...|       4.0|\n",
      "|[-18.722712558763...|            3|[285.0,225.0,235....|[0.18566775244299...|       4.0|\n",
      "|[1.35096682912810...|            4|[285.0,225.0,235....|[0.18566775244299...|       4.0|\n",
      "|[155.893543127723...|            2|[806.0,738.0,535....|[0.28380281690140...|       0.0|\n",
      "|[-39.455378121526...|            3|[40.0,38.0,83.0,1...|[0.05970149253731...|       4.0|\n",
      "|[-41.490887225481...|            4|[40.0,38.0,83.0,1...|[0.05970149253731...|       4.0|\n",
      "|[248.519470169715...|            2|[22.0,78.0,58.0,6...|[0.08088235294117...|       1.0|\n",
      "|[-41.200100210630...|            3|[40.0,38.0,83.0,1...|[0.05970149253731...|       4.0|\n",
      "|[-42.847893294784...|            1|[55.0,9.0,6.0,6.0...|[0.69620253164556...|       0.0|\n",
      "|[17.6113386636624...|            3|[285.0,225.0,235....|[0.18566775244299...|       4.0|\n",
      "|[-20.467434647867...|            2|[138.0,58.0,43.0,...|[0.44516129032258...|       0.0|\n",
      "|[175.217791549063...|            3|[806.0,738.0,535....|[0.28380281690140...|       0.0|\n",
      "|[-42.654035284884...|            1|[55.0,9.0,6.0,6.0...|[0.69620253164556...|       0.0|\n",
      "|[-42.847893294784...|            1|[55.0,9.0,6.0,6.0...|[0.69620253164556...|       0.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       0.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            1|       0.0|\n",
      "|            4|       0.0|\n",
      "|            2|       0.0|\n",
      "|            4|       4.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       1.0|\n",
      "|            3|       4.0|\n",
      "|            1|       0.0|\n",
      "|            3|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       0.0|\n",
      "|            1|       0.0|\n",
      "|            1|       0.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.123\n",
      "+------------------------+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|4.0|\n",
      "+------------------------+---+---+---+\n",
      "|                       3|524| 12|106|\n",
      "|                       0| 57|  5| 19|\n",
      "|                       1|502| 35| 82|\n",
      "|                       4|547| 18|258|\n",
      "|                       2|698| 21|115|\n",
      "+------------------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Split the data into training and validation data\n",
    "train_data = finalized_data\n",
    "classifier = DecisionTreeClassifier( labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-0.4646831918769...|            4|[4.47205142084630...|[0.22360257104231...|       4.0|\n",
      "|[0.18781876972525...|            4|[3.83380533287242...|[0.19169026664362...|       4.0|\n",
      "|[-0.9753292896775...|            2|[3.83380533287242...|[0.19169026664362...|       4.0|\n",
      "|[74.3435237231082...|            1|[7.28481869753369...|[0.36424093487668...|       0.0|\n",
      "|[-1.8476903342297...|            4|[5.85447670750173...|[0.29272383537508...|       0.0|\n",
      "|[-41.781674240331...|            2|[4.82163629683130...|[0.24108181484156...|       4.0|\n",
      "|[47.1869516866008...|            4|[7.28481869753369...|[0.36424093487668...|       0.0|\n",
      "|[-18.722712558763...|            3|[4.09998258913485...|[0.20499912945674...|       4.0|\n",
      "|[1.35096682912810...|            4|[3.28048052829982...|[0.16402402641499...|       4.0|\n",
      "|[155.893543127723...|            2|[5.79997720937876...|[0.28999886046893...|       0.0|\n",
      "|[-39.455378121526...|            3|[1.672370288935,1...|[0.08361851444675...|       4.0|\n",
      "|[-41.490887225481...|            4|[2.99030835351836...|[0.14951541767591...|       4.0|\n",
      "|[248.519470169715...|            2|[2.52013164823300...|[0.12600658241165...|       1.0|\n",
      "|[-41.200100210630...|            3|[2.99030835351836...|[0.14951541767591...|       4.0|\n",
      "|[-42.847893294784...|            1|[5.11931288473064...|[0.25596564423653...|       2.0|\n",
      "|[17.6113386636624...|            3|[3.24159978538682...|[0.16207998926934...|       4.0|\n",
      "|[-20.467434647867...|            2|[5.09019687876670...|[0.25450984393833...|       4.0|\n",
      "|[175.217791549063...|            3|[5.79997720937876...|[0.28999886046893...|       0.0|\n",
      "|[-42.654035284884...|            1|[10.1934672097794...|[0.50967336048897...|       0.0|\n",
      "|[-42.847893294784...|            1|[5.11931288473064...|[0.25596564423653...|       2.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       4.0|\n",
      "|            1|       0.0|\n",
      "|            4|       0.0|\n",
      "|            2|       4.0|\n",
      "|            4|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       0.0|\n",
      "|            3|       4.0|\n",
      "|            4|       4.0|\n",
      "|            2|       1.0|\n",
      "|            3|       4.0|\n",
      "|            1|       2.0|\n",
      "|            3|       4.0|\n",
      "|            2|       4.0|\n",
      "|            3|       0.0|\n",
      "|            1|       0.0|\n",
      "|            1|       2.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "F1 score: 0.208\n",
      "+------------------------+---+---+---+---+---+\n",
      "|AdoptionSpeed_prediction|0.0|1.0|2.0|3.0|4.0|\n",
      "+------------------------+---+---+---+---+---+\n",
      "|                       3|394| 20| 65| 14|149|\n",
      "|                       0| 51|  7|  4|  0| 19|\n",
      "|                       1|417| 56| 67| 11| 68|\n",
      "|                       4|397| 21| 36| 21|348|\n",
      "|                       2|559| 29| 86| 18|142|\n",
      "+------------------------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "train_data = finalized_data\n",
    "classifier = RandomForestClassifier(labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "\n",
    "\n",
    "test_data = testfinalized_data\n",
    "results = classifier.transform(test_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "\n",
    "# Want to show the f1 score and confusion matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"AdoptionSpeed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(results)\n",
    "print(\"F1 score: %.3f\" % f1_score)\n",
    "\n",
    "# Confusion matrix\n",
    "results.crosstab('AdoptionSpeed', 'prediction').show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
