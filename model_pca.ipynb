{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting the libraries\n",
    "import pyspark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark to read the data and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with spark we need to create a spark session\n",
    "# Need to instal java\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('petfinder').getOrCreate()\n",
    "# Read a dataset with spark\n",
    "df_spark = spark.read.csv('./train_balanced_corr_pca.csv', header=True, inferSchema=True)\n",
    "# Header = True, inferSchema = True, means that the first row is the header and the schema is inferred (if schema is not inferred, all columns will be read as string)\n",
    "# Convert the column \"AdoptionSpeed\" to integer\n",
    "df_spark = df_spark.withColumn(\"AdoptionSpeed\", df_spark[\"AdoptionSpeed\"].cast(\"integer\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PySpark MLlib to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+--------------------+\n",
      "|                  0|                   1|AdoptionSpeed|            features|\n",
      "+-------------------+--------------------+-------------+--------------------+\n",
      "| -37.16037461817183| -0.4456896700259529|            2|[-37.160374618171...|\n",
      "| -4.160398248205791|-0.47001473885723066|            3|[-4.1603982482057...|\n",
      "|-1.1604003963906866|-0.47222610875098475|            2|[-1.1604003963906...|\n",
      "| -45.15867983673255|  0.4996916193946927|            1|[-45.158679836732...|\n",
      "| -45.15867983673255|  0.4996916193946927|            2|[-45.158679836732...|\n",
      "| -45.15867983673255|  0.4996916193946927|            0|[-45.158679836732...|\n",
      "| -45.15947110531252| -0.4985787143538656|            1|[-45.159471105312...|\n",
      "|-2.1588171431691077|  1.5250516820440492|            3|[-2.1588171431691...|\n",
      "|-3.1596076956874497|  0.5275184715934086|            2|[-3.1596076956874...|\n",
      "| -44.15957833716055|   0.557740526808022|            0|[-44.159578337160...|\n",
      "| -45.15947110531252| -0.4985787143538656|            2|[-45.159471105312...|\n",
      "| -4.159606979625818|   0.528255594891326|            2|[-4.1596069796258...|\n",
      "| -45.15947110531252| -0.4985787143538656|            2|[-45.159471105312...|\n",
      "| -4.160398248205791| -0.4700147388572322|            2|[-4.1603982482057...|\n",
      "|  54.84124855710427| 0.42597928960295267|            1|[54.8412485571042...|\n",
      "| -45.15867983673255|  0.4996916193946927|            4|[-45.158679836732...|\n",
      "| -45.15867983673255|  0.4996916193946927|            1|[-45.158679836732...|\n",
      "| -45.15947110531252| -0.4985787143538656|            2|[-45.159471105312...|\n",
      "| -4.160398248205791| -0.4700147388572322|            1|[-4.1603982482057...|\n",
      "| -4.160398248205791| -0.4700147388572322|            1|[-4.1603982482057...|\n",
      "+-------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureassemble = VectorAssembler(inputCols=['0','1'], outputCol='features')\n",
    "output = featureassemble.transform(df_spark) # This will create a new column called 'features' which is a vector of the selected columns (Type, Age2, Breed1) by the VectorAssembler\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|AdoptionSpeed|\n",
      "+--------------------+-------------+\n",
      "|[-37.160374618171...|            2|\n",
      "|[-4.1603982482057...|            3|\n",
      "|[-1.1604003963906...|            2|\n",
      "|[-45.158679836732...|            1|\n",
      "|[-45.158679836732...|            2|\n",
      "|[-45.158679836732...|            0|\n",
      "|[-45.159471105312...|            1|\n",
      "|[-2.1588171431691...|            3|\n",
      "|[-3.1596076956874...|            2|\n",
      "|[-44.159578337160...|            0|\n",
      "|[-45.159471105312...|            2|\n",
      "|[-4.1596069796258...|            2|\n",
      "|[-45.159471105312...|            2|\n",
      "|[-4.1603982482057...|            2|\n",
      "|[54.8412485571042...|            1|\n",
      "|[-45.158679836732...|            4|\n",
      "|[-45.158679836732...|            1|\n",
      "|[-45.159471105312...|            2|\n",
      "|[-4.1603982482057...|            1|\n",
      "|[-4.1603982482057...|            1|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data = output.select('features', 'AdoptionSpeed') # Select the features and the target column\n",
    "finalized_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-45.159577621098...|            4|[0.20595184275855...|[0.24352631590512...|       0.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "|[-45.159471105312...|            0|[-0.2940410804854...|[0.14659358017979...|       4.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       0.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23835161183117315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Split the data into training and validation data\n",
    "train_data, validation_data = finalized_data.randomSplit([0.1, 0.9])\n",
    "classifier = LogisticRegression\n",
    "classifier = LogisticRegression(labelCol='AdoptionSpeed').fit(train_data) # Fit the model\n",
    "results = classifier.evaluate(validation_data) # Evaluate the model on the validation data\n",
    "results.predictions.show() # Show the predictions\n",
    "results.predictions.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "results.accuracy # Show the accuracy of the model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-45.159577621098...|            4|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "|[-45.159471105312...|            0|[25.0,54.0,81.0,8...|[0.07062146892655...|       4.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24862208645992429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Split the data into training and validation data\n",
    "train_data, validation_data = finalized_data.randomSplit([0.1, 0.9])\n",
    "classifier = DecisionTreeClassifier( labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "results = classifier.transform(validation_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "# results.accuracy # Show the accuracy of the model\n",
    "results.accuracy = results.filter(results.AdoptionSpeed == results.prediction).count() / float(results.count())\n",
    "results.accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|AdoptionSpeed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-45.159577621098...|            4|[1.41873724001744...|[0.07093686200087...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "|[-45.159471105312...|            0|[1.28397977367512...|[0.06419898868375...|       4.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|AdoptionSpeed|prediction|\n",
      "+-------------+----------+\n",
      "|            4|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "|            0|       4.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24893814706663128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Split the data into training and validation data\n",
    "train_data, validation_data = finalized_data.randomSplit([0.1, 0.9])\n",
    "classifier = RandomForestClassifier( labelCol='AdoptionSpeed', featuresCol='features')\n",
    "classifier = classifier.fit(train_data) # Fit the model\n",
    "results = classifier.transform(validation_data) # Evaluate the model on the validation data\n",
    "results.show() # Show the predictions\n",
    "results.select('AdoptionSpeed', 'prediction').show() # Show the target and the prediction\n",
    "# results.accuracy # Show the accuracy of the model\n",
    "results.accuracy = results.filter(results.AdoptionSpeed == results.prediction).count() / float(results.count())\n",
    "results.accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
